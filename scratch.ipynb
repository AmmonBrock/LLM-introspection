{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1356c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ammonbro/advanced-deep-learning/final_project/.venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from get_model import get_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a2ec384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_concept_vectors(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    concept_words,\n",
    "    layer_idx,\n",
    "    batch_size=8,\n",
    "):\n",
    "    prompt = \"Tell me about {word}.\"\n",
    "\n",
    "\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output[0].detach()\n",
    "        return hook\n",
    "    \n",
    "    model.model.layers[layer_idx].register_forward_hook(get_activation('target_activation'))\n",
    "    concept_vectors = []\n",
    "    activations_list = []\n",
    "    for word in concept_words:\n",
    "        assert len(activation) == 0, \"Activations dict should be empty before forward pass.\"\n",
    "        message = [{\"role\": \"user\", \"content\": prompt.format(word=word)}]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize = False,\n",
    "            add_generation_prompt = True\n",
    "        ) \n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**model_inputs)\n",
    "        activations_list.append(activation.pop('target_activation')[-1, :].squeeze(0).cpu()) # Test this\n",
    "\n",
    "\n",
    "\n",
    "    model.model.layers[layer_idx].register_forward_hook(None)\n",
    "    return activations_list, concept_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. UTILITY FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def save_concept_vectors(concept_vectors, concept_labels, filepath):\n",
    "    \"\"\"Save concept vectors to disk\"\"\"\n",
    "    torch.save({\n",
    "        'vectors': torch.stack(concept_vectors),\n",
    "        'labels': concept_labels,\n",
    "    }, filepath)\n",
    "    print(f\"✓ Saved concept vectors to {filepath}\")\n",
    "\n",
    "\n",
    "def load_concept_vectors(filepath):\n",
    "    \"\"\"Load concept vectors from disk\"\"\"\n",
    "    data = torch.load(filepath)\n",
    "    vectors = [data['vectors'][i] for i in range(len(data['vectors']))]\n",
    "    labels = data['labels']\n",
    "    return vectors, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b9208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 28.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Model class: <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'>\n",
      "Local Tokenizer class: <class 'transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "736a14ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using layer 23 out of 36\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(model.model.layers)\n",
    "layer_idx = int(num_layers * 0.66)\n",
    "print(f\"Using layer {layer_idx} out of {num_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "111d192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.5820, -0.6562,  0.8281,  ...,  0.8125,  2.7344, -0.6016],\n",
      "       dtype=torch.bfloat16), tensor([ 0.6094, -1.0000,  0.6641,  ...,  0.7500,  2.5625, -0.8086],\n",
      "       dtype=torch.bfloat16)]\n",
      "['dust', 'satellites']\n"
     ]
    }
   ],
   "source": [
    "concept_words = [\n",
    "    # From the paper's list\n",
    "    \"dust\", \"satellites\", #\"trumpets\", \"origami\", \"illusions\",\n",
    "    # \"cameras\", \"lightning\", \"constellations\", \"treasures\", \"phones\",\n",
    "    # \"trees\", \"avalanches\", \"mirrors\", \"fountains\", \"quarries\",\n",
    "    # \"sadness\", \"xylophones\", \"secrecy\", \"oceans\", \"information\",\n",
    "    # \"deserts\", \"kaleidoscopes\", \"sugar\", \"vegetables\", \"poetry\",\n",
    "    # \"aquariums\", \"bags\", \"peace\", \"caverns\", \"memories\",\n",
    "    # \"frosts\", \"volcanoes\", \"boulders\", \"harmonies\", \"masquerades\",\n",
    "    # \"rubber\", \"plastic\", \"blood\", \"amphitheaters\", \"contraptions\",\n",
    "    # \"youths\", \"dynasties\", \"snow\", \"dirigibles\", \"algorithms\",\n",
    "    # \"denim\", \"monoliths\", \"milk\", \"bread\", \"silver\",\n",
    "]\n",
    "\n",
    "vectors, labels = extract_concept_vectors(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    concept_words=concept_words,\n",
    "    layer_idx=layer_idx,\n",
    "    batch_size=1,\n",
    ")\n",
    "print(vectors)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22fdfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about {word}.\"\n",
    "\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output[0].detach()\n",
    "    return hook\n",
    "\n",
    "model.model.layers[layer_idx].register_forward_hook(get_activation('target_activation'))\n",
    "concept_vectors = []\n",
    "activations_list = []\n",
    "for word in concept_words:\n",
    "    assert len(activation) == 0, \"Activations dict should be empty before forward pass.\"\n",
    "    message = [{\"role\": \"user\", \"content\": prompt.format(word=word)}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True\n",
    "    ) \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(**model_inputs)\n",
    "    break\n",
    "    activations_list.append(activation.pop('target_activation')[-1, :].cpu()) \n",
    "\n",
    "\n",
    "\n",
    "#model.model.layers[layer_idx].register_forward_hook(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a9ae709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5820, -0.6562,  0.8281,  ...,  0.8125,  2.7344, -0.6016],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation['target_activation'][-1,:].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d288fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adjust_missing_and_unexpected_keys',\n",
       " '_apply',\n",
       " '_assisted_decoding',\n",
       " '_auto_class',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_beam_search',\n",
       " '_beam_search_has_unfinished_sequences',\n",
       " '_buffers',\n",
       " '_cache_dependant_input_preparation',\n",
       " '_cache_dependant_input_preparation_exporting',\n",
       " '_call_impl',\n",
       " '_can_compile_fullgraph',\n",
       " '_can_record_outputs',\n",
       " '_can_set_attn_implementation',\n",
       " '_check_and_adjust_attn_implementation',\n",
       " '_check_early_stop_heuristic',\n",
       " '_checkpoint_conversion_mapping',\n",
       " '_compiled_call_impl',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_copy_lm_head_original_to_resized',\n",
       " '_create_repo',\n",
       " '_dispatch_accelerate_model',\n",
       " '_ep_plan',\n",
       " '_expand_inputs_for_generation',\n",
       " '_extract_generation_mode_kwargs',\n",
       " '_fix_state_dict_key_on_load',\n",
       " '_fix_state_dict_key_on_save',\n",
       " '_fix_state_dict_keys_on_save',\n",
       " '_flash_attn_2_can_dispatch',\n",
       " '_flash_attn_3_can_dispatch',\n",
       " '_flatten_beam_dim',\n",
       " '_flex_attn_can_dispatch',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_from_config',\n",
       " '_gather_beams',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_cache',\n",
       " '_get_candidate_generator',\n",
       " '_get_deprecated_gen_repo',\n",
       " '_get_files_timestamps',\n",
       " '_get_initial_cache_position',\n",
       " '_get_key_renaming_mapping',\n",
       " '_get_logits_processor',\n",
       " '_get_name',\n",
       " '_get_no_split_modules',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_running_beams_for_next_iteration',\n",
       " '_get_stopping_criteria',\n",
       " '_get_top_k_continuations',\n",
       " '_has_unfinished_sequences',\n",
       " '_hf_peft_config_loaded',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_added_embeddings_weights_with_mean',\n",
       " '_init_added_lm_head_bias_with_mean',\n",
       " '_init_added_lm_head_weights_with_mean',\n",
       " '_init_weights',\n",
       " '_initialize_missing_keys',\n",
       " '_initialize_weights',\n",
       " '_input_embed_layer',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_is_stateful',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keep_in_fp32_modules_strict',\n",
       " '_keep_in_fp32_modules_strict',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_flax',\n",
       " '_load_from_state_dict',\n",
       " '_load_from_tf',\n",
       " '_load_pretrained_model',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_initialize_input_ids_for_generation',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_modules',\n",
       " '_move_missing_keys_from_meta_to_cpu',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_pp_plan',\n",
       " '_pp_plan',\n",
       " '_prefill_chunking',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_cache_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_generated_length',\n",
       " '_prepare_generation_config',\n",
       " '_prepare_model_inputs',\n",
       " '_prepare_special_tokens',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_sample',\n",
       " '_save_to_state_dict',\n",
       " '_sdpa_can_dispatch',\n",
       " '_set_default_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_skip_keys_device_placement',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_supports_attention_backend',\n",
       " '_supports_default_dynamic_cache',\n",
       " '_supports_flash_attn',\n",
       " '_supports_flex_attn',\n",
       " '_supports_logits_to_keep',\n",
       " '_supports_sdpa',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_tied_weights_keys',\n",
       " '_tp_plan',\n",
       " '_tp_plan',\n",
       " '_tp_size',\n",
       " '_unflatten_beam_dim',\n",
       " '_update_finished_beams',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_upload_modified_files',\n",
       " '_valid_auto_compile_criteria',\n",
       " '_validate_generated_length',\n",
       " '_validate_generation_mode',\n",
       " '_validate_model_kwargs',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'active_adapters',\n",
       " 'add_adapter',\n",
       " 'add_memory_hooks',\n",
       " 'add_model_tags',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'can_generate',\n",
       " 'can_record_outputs',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'compute_transition_scores',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'delete_adapter',\n",
       " 'dequantize',\n",
       " 'device',\n",
       " 'disable_adapters',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'enable_adapters',\n",
       " 'enable_input_require_grads',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'generate_batch',\n",
       " 'generation_config',\n",
       " 'get_adapter_state_dict',\n",
       " 'get_buffer',\n",
       " 'get_compiled_call',\n",
       " 'get_correct_attn_implementation',\n",
       " 'get_decoder',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_init_context',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_parameter_or_buffer',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'half',\n",
       " 'heal_tokens',\n",
       " 'hf_device_map',\n",
       " 'init_continuous_batching',\n",
       " 'init_weights',\n",
       " 'initialize_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_backend_compatible',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_parallelizable',\n",
       " 'kernelize',\n",
       " 'lm_head',\n",
       " 'load_adapter',\n",
       " 'load_custom_generate',\n",
       " 'load_state_dict',\n",
       " 'loss_function',\n",
       " 'loss_type',\n",
       " 'main_input_name',\n",
       " 'model',\n",
       " 'model_tags',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'post_init',\n",
       " 'pp_plan',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'reverse_bettertransformer',\n",
       " 'save_pretrained',\n",
       " 'set_adapter',\n",
       " 'set_attn_implementation',\n",
       " 'set_decoder',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'set_output_embeddings',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'smart_apply',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'supports_pp_plan',\n",
       " 'supports_tp_plan',\n",
       " 'tie_embeddings_and_encoder_decoder',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_bettertransformer',\n",
       " 'to_empty',\n",
       " 'tp_plan',\n",
       " 'tp_size',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'use_kernels',\n",
       " 'vocab_size',\n",
       " 'warn_if_padding_and_no_attention_mask',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
